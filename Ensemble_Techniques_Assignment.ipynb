{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4oEE3Uuy-hhh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP3Z_1LX-bBg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theory**"
      ],
      "metadata": {
        "id": "4oEE3Uuy-hhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Can we use Bagging for regression problems?**\n",
        "- Yes, we can use Bagging for regression problems."
      ],
      "metadata": {
        "id": "T3YbQUJsulyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **What is the difference between multiple model training and single model training?**\n",
        "- Single Model Training\n",
        "  - Uses only one model to learn patterns from the data and make predictions.\n",
        "  - Simpler to implement and requires less computational power.\n",
        "  - Works well when the dataset is not too complex or when one algorithm is sufficient.\n",
        "  - Example: Training a single Decision Tree to classify customer churn.\n",
        "- Multiple Model Training\n",
        "  - Uses multiple models (either different types of models or the same model trained with different variations).\n",
        "  - Improves accuracy and robustness by combining predictions from multiple models.\n",
        "  - Often used in ensemble learning techniques like Bagging, Boosting, and Stacking.\n",
        "  - Example: Training multiple Decision Trees (Bagging Classifier) or combining SVM, Naive Bayes, and Random Forest in a voting system.\n"
      ],
      "metadata": {
        "id": "JBIEZBc0cHl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Explain the concept of feature randomness in Random Forest.**\n",
        "- Feature randomness in Random Forest refers to the technique where each decision tree in the forest is trained on a random subset of features, rather than using all features. This helps increase diversity among trees and improves the generalization ability of the model.\n",
        "- Working:\n",
        "  - nstead of considering all features at every split, each tree selects a random subset of features.\n",
        "  - his prevents dominant features from always being chosen, ensuring variety.\n",
        "  - Since different trees look at different features, they make different decisions, reducing correlation between trees.\n",
        "  - Helps in handling noisy features more effectively.\n",
        "  - Ensures the model doesn’t rely too much on a particular feature or pattern.\n",
        "  - Improves stability and accuracy when making predictions on new data."
      ],
      "metadata": {
        "id": "qRZCb-IJcgec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **What is OOB (Out-of-Bag) Score?**\n",
        "- The Out-of-Bag (OOB) Score is a method used in Random Forest to evaluate model performance without needing a separate validation set. It measures how well the model generalizes by using bootstrapped samples during training."
      ],
      "metadata": {
        "id": "lL0__XwD2c8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **How can you measure the importance of features in a Random Forest model?**\n",
        "-  Mean Decrease in Impurity (Gini Importance)\n",
        "  - Random Forest uses Decision Trees, which split nodes based on feature values to reduce impurity (Gini index or entropy).\n",
        "  - After training, it calculates how much impurity is reduced by each feature across all trees.\n",
        "  - Features with higher impurity reduction are more important.\n",
        "- Mean Decrease in Accuracy (Permutation Importance)\n",
        "  - Measures how much accuracy drops when a feature's values are randomly shuffled.\n",
        "  - If accuracy decreases significantly, the feature is important."
      ],
      "metadata": {
        "id": "Fbq9Pr3s2tYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Explain the working principle of a Bagging Classifier.**\n",
        "- The Bagging Classifier (short for Bootstrap Aggregating) is an ensemble learning technique that improves the accuracy and stability of machine learning models, particularly reducing variance and overfitting. It works by training multiple base models (usually Decision Trees) on different subsets of the training data and averaging their predictions.\n",
        "- Working Principle of Bagging Classifier\n",
        "  - Bootstrap Sampling:\n",
        "    - The training data is randomly sampled with replacement, creating multiple different subsets.\n",
        "    - Some samples may be duplicated, while others may not be selected at all.\n",
        "  - Independent Model Training:\n",
        "    - A base estimator (e.g., Decision Tree, SVM, etc.) is trained on each subset separately.\n",
        "    - Each model learns different patterns, increasing diversity in predictions.\n",
        "  - Aggregation of Predictions:\n",
        "    - For classification: Predictions are combined using majority voting.\n"
      ],
      "metadata": {
        "id": "mvl8WW_W4T0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **How do you evaluate a Bagging Classifier's performance?**\n",
        "- Accuracy & Error Metrics\n",
        "  - Accuracy Score → Measures overall correctness.\n",
        "  - Precision, Recall, F1-score → Determines class-specific performance.\n",
        "- Out-of-Bag (OOB) Score\n",
        "  - Bagging uses bootstrap sampling, leaving out some data in each model.\n",
        "  - OOB samples can serve as a validation set without splitting data manually.\n",
        "  - Enables fast error estimation:\n",
        "- Cross-Validation\n",
        "  - Evaluates the model over multiple train-test splits, ensuring stability.\n",
        "  - Prevents overfitting while validating true performance.\n",
        "- Bias-Variance Analysis\n",
        "  - Bagging reduces variance by averaging multiple weak models.\n",
        "  - Checking train vs. test accuracy helps analyze bias-variance tradeoff."
      ],
      "metadata": {
        "id": "2zHyZH3M8DVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **How does a Bagging Regressor work?**\n",
        "- - Working Principle of Bagging Regressor\n",
        "  - Bootstrap Sampling:\n",
        "    - The training data is randomly sampled with replacement, creating multiple different subsets.\n",
        "    - Some samples may be duplicated, while others may not be selected at all.\n",
        "  - Independent Model Training:\n",
        "    - A base estimator (e.g., Decision Tree, SVM, etc.) is trained on each subset separately.\n",
        "    - Each model learns different patterns, increasing diversity in predictions.\n",
        "  - Aggregation of Predictions:\n",
        "    - For regression: Predictions are averaged to reduce variance."
      ],
      "metadata": {
        "id": "dpwXltqk8xbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **What is the main advantage of ensemble techniques?**\n",
        "- Key Benefits of Ensemble Techniques\n",
        "  - Higher Accuracy: Aggregating multiple weak models often leads to better performance than a single model.\n",
        "  - Reduces Overfitting: Since models train on different subsets of data, ensemble learning prevents over-reliance on specific patterns.\n",
        "  - Handles Noisy Data Better: Mistakes from individual models are averaged out.\n",
        "  - Improves Model Stability: Reduces sensitivity to small variations in training data.\n",
        "  - Balances Bias-Variance Tradeoff: Helps optimize performance across different datasets."
      ],
      "metadata": {
        "id": "J-vPuXFJ9HaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **What is the main challenge of ensemble methods?**\n",
        "- Main Challenges of Ensemble Methods\n",
        "  - Training multiple models simultaneously requires more time and resources. Some ensemble techniques, like Boosting, iterate multiple times to refine predictions.\n",
        "  - Simple models like Decision Trees are easy to interpret, but ensemble methods combine many models, making results harder to explain. This is a problem in fields like medicine or finance, where explainability is critical.\n",
        "  - If ensemble methods are too complex or trained on noisy data, they may overfit instead of improving generalization. Boosting methods, in particular, can amplify noise if not carefully tuned.\n",
        "  - Techniques like Random Forest, XGBoost, and Stacking require careful tuning of parameters like n_estimators, learning rates, and model depth. Poor tuning can lead to inefficient or suboptimal models.\n",
        "  - A single model is easy to deploy in production, while ensemble models require additional infrastructure. Large-scale applications must balance between performance and deployment complexity."
      ],
      "metadata": {
        "id": "3vi66RR39XEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. **Explain the key idea behind ensemble techniques.**\n",
        "- The key idea behind ensemble techniques is to combine multiple models to improve accuracy, stability, and generalization. Instead of relying on a single classifier or regressor, ensemble methods leverage multiple models to reduce bias, variance, and overfitting, leading to stronger predictions."
      ],
      "metadata": {
        "id": "7ak1Glbr90w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. **What is a Random Forest Classifier?**\n",
        "- A Random Forest Classifier is an ensemble learning algorithm that builds multiple decision trees and combines their outputs to improve accuracy and prevent overfitting. It's widely used for classification tasks because it balances bias and variance effectively.\n",
        "- Key Benefits\n",
        "  - Higher accuracy than a single Decision Tree.\n",
        "  - Handles missing values & noisy data better.\n",
        "  - Works well with large datasets.\n",
        "  - Robust against outliers."
      ],
      "metadata": {
        "id": "uXMkH_Ik-A46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. **What are the main types of ensemble techniques?**\n",
        "- Ensemble techniques enhance machine learning models by combining multiple predictors to improve accuracy, reduce variance, and enhance generalization. The main types of ensemble techniques include:\n",
        "  - Bagging (Bootstrap Aggregating)\n",
        "  - Boosting\n",
        "  - Stacking (Stacked Generalization)\n",
        "  - Voting & Averaging"
      ],
      "metadata": {
        "id": "1E-xFMWJ-UMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. **What is ensemble learning in machine learning?**\n",
        "- Ensemble learning is a powerful machine learning technique where multiple models are combined to improve accuracy, stability, and generalization. Instead of relying on a single predictor, ensemble methods harness the strengths of multiple models to make more robust predictions."
      ],
      "metadata": {
        "id": "n589tXIr-2kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. **When should we avoid using ensemble methods?**\n",
        "- While ensemble methods are powerful, there are certain situations where they may not be the best choice. Here’s when you should avoid using ensemble techniques:\n",
        "  - When Interpretability is Crucial.\n",
        "  - When Computational Resources are Limited.\n",
        "  - When a Simple Model is Already Performing Well.\n",
        "  - When You Have Limited Data.\n",
        "  - When Deployment Complexity is a Concern."
      ],
      "metadata": {
        "id": "ys_UiueT-7k6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. **How does Bagging help in reducing overfitting?**\n",
        "- Creates Diverse Models:\n",
        "  - Each model is trained on a random subset of the data, ensuring they learn different patterns.\n",
        "  - This prevents any single model from memorizing specific training data.\n",
        "- Reduces Model Variance:\n",
        "  - Individual models (like Decision Trees) tend to overfit due to their high variance.\n",
        "  - Bagging averages their predictions, leading to a more stable model.\n",
        "- Helps Generalization:\n",
        "  - By training models on different samples, Bagging ensures the overall classifier doesn’t depend too much on specific noisy data points.\n",
        "  - The final model generalizes better to unseen data.\n",
        "- Majority Voting or Averaging:\n",
        "  - In classification, Bagging combines predictions using majority voting, balancing errors from different models.\n",
        "  - In regression, predictions are averaged, smoothing out noise."
      ],
      "metadata": {
        "id": "E5skzQrf_avL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. **Why is Random Forest better than a single Decision Tree?**\n",
        "- Key Advantages of Random Forest Over Decision Trees\n",
        "  - Reduces Overfitting.\n",
        "  - Improves Accuracy.\n",
        "  - Handles Feature Variability.\n",
        "  - Works Well with Large Datasets.\n",
        "  - Reduces Sensitivity to Outliers."
      ],
      "metadata": {
        "id": "gOcuc1fk_1Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. **What is the role of bootstrap sampling in Bagging?**\n",
        "- Generates multiple subsets of data by randomly selecting samples with replacement.\n",
        "- Ensures diversity among models by providing different data distributions to each base learner.\n",
        "- Reduces overfitting by averaging predictions across multiple weak models.\n",
        "- Improves stability by preventing reliance on specific data points, making the model robust to noise."
      ],
      "metadata": {
        "id": "c53Jn_CzANNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. **What are some real-world applications of ensemble techniques?**\n",
        "- Fraud Detection.\n",
        "- Healthcare & Disease Prediction.\n",
        "- Spam Detection & Email Filtering.\n",
        "- Stock Market & Financial Predictions.\n",
        "- Sentiment Analysis & Natural Language Processing (NLP).\n",
        "- Image Recognition & Object Detection.\n",
        "- Customer Churn Prediction in Businesses.\n",
        "- Cybersecurity Threat Detection.\n",
        "- Medical Imaging & Radiology Analysis."
      ],
      "metadata": {
        "id": "zgJlOcMMAdWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. **What is the difference between Bagging and Boosting?**\n",
        "- Bagging (Bootstrap Aggregating)\n",
        "  - Goal: Reduce variance and prevent overfitting.\n",
        "  - How It Works:\n",
        "    - Creates multiple models trained on different bootstrapped subsets of data.\n",
        "  - Models are trained independently and predictions are aggregated (majority voting for classification, averaging for regression).\n",
        "  - Example Algorithms:\n",
        "    - Random Forest (multiple Decision Trees trained separately).\n",
        "  - Best for: High-variance models like Decision Trees, where averaging multiple models stabilizes predictions.\n",
        "\n",
        "- Boosting\n",
        "  - Goal: Reduce bias by focusing on misclassified samples.\n",
        "  - How It Works:\n",
        "    - Models are trained sequentially, each improving upon the mistakes of the previous one.\n",
        "  - Weak learners are combined into a stronger model by giving more weight to misclassified examples.\n",
        "  - Example Algorithms:\n",
        "    - AdaBoost (Adjusts weights based on errors).\n",
        "    - XGBoost (Optimized gradient boosting for high efficiency).\n",
        "  - Best for: Handling complex patterns, where models learn progressively to refine predictions."
      ],
      "metadata": {
        "id": "p3bI06hSAzfW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZbpaAGle-j7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical**"
      ],
      "metadata": {
        "id": "976o2l3Ouwpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy."
      ],
      "metadata": {
        "id": "MiGzPl3Suz4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Training of Dataset\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "bgc = BaggingClassifier(estimator = DecisionTreeClassifier(), n_estimators=15, random_state=1)\n",
        "bgc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = bgc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The accuracy of the model is {accuracy_score(y_test, y_pred)}.\")"
      ],
      "metadata": {
        "id": "UTZHBLQ-uzLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5938af-99c7-4b21-a00b-94ef7cb69d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is 0.876.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "u1RVRJerajGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_regression\n",
        "x, y = make_regression(n_samples=1000, n_features=10, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Training of Dataset\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "bgc = BaggingRegressor(estimator = DecisionTreeRegressor(), n_estimators=15, random_state=1)\n",
        "bgc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = bgc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(f\"The mean_squared_error of the model is {mean_squared_error(y_test, y_pred)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TETtrhsaRpM",
        "outputId": "96a83ce2-a827-47e5-aa97-38c06f739e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean_squared_error of the model is 4828.8301122928115.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
      ],
      "metadata": {
        "id": "p6Lkpbn0bGdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "dt = load_breast_cancer()\n",
        "y = dt.target\n",
        "x = pd.DataFrame(dt.data, columns = dt.feature_names)\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "# Model Testing\n",
        "y_pred = rf.predict(x_test)\n",
        "\n",
        "# Feature_importance_Score\n",
        "\n",
        "df = {\"Features\": x.columns,\n",
        " \"Feature Imporance Score\": rf.feature_importances_}\n",
        "df = pd.DataFrame(df)\n",
        "df"
      ],
      "metadata": {
        "id": "7_-4W8-Oa7hc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "7dfc8f63-f5b4-47f2-989e-eb8a8683257a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Features  Feature Imporance Score\n",
              "0               mean radius                 0.040764\n",
              "1              mean texture                 0.015682\n",
              "2            mean perimeter                 0.051730\n",
              "3                 mean area                 0.036748\n",
              "4           mean smoothness                 0.005140\n",
              "5          mean compactness                 0.017157\n",
              "6            mean concavity                 0.030737\n",
              "7       mean concave points                 0.100411\n",
              "8             mean symmetry                 0.002802\n",
              "9    mean fractal dimension                 0.003761\n",
              "10             radius error                 0.015226\n",
              "11            texture error                 0.002007\n",
              "12          perimeter error                 0.018451\n",
              "13               area error                 0.028475\n",
              "14         smoothness error                 0.003819\n",
              "15        compactness error                 0.002594\n",
              "16          concavity error                 0.003501\n",
              "17     concave points error                 0.006780\n",
              "18           symmetry error                 0.002520\n",
              "19  fractal dimension error                 0.004063\n",
              "20             worst radius                 0.105003\n",
              "21            worst texture                 0.022585\n",
              "22          worst perimeter                 0.134225\n",
              "23               worst area                 0.113043\n",
              "24         worst smoothness                 0.012033\n",
              "25        worst compactness                 0.016953\n",
              "26          worst concavity                 0.046426\n",
              "27     worst concave points                 0.143496\n",
              "28           worst symmetry                 0.007263\n",
              "29  worst fractal dimension                 0.006607"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd344581-df23-4ce8-8862-b6cf0cf8dfd8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Features</th>\n",
              "      <th>Feature Imporance Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mean radius</td>\n",
              "      <td>0.040764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mean texture</td>\n",
              "      <td>0.015682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mean perimeter</td>\n",
              "      <td>0.051730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mean area</td>\n",
              "      <td>0.036748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mean smoothness</td>\n",
              "      <td>0.005140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>mean compactness</td>\n",
              "      <td>0.017157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mean concavity</td>\n",
              "      <td>0.030737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mean concave points</td>\n",
              "      <td>0.100411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>mean symmetry</td>\n",
              "      <td>0.002802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>mean fractal dimension</td>\n",
              "      <td>0.003761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>radius error</td>\n",
              "      <td>0.015226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>texture error</td>\n",
              "      <td>0.002007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>perimeter error</td>\n",
              "      <td>0.018451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>area error</td>\n",
              "      <td>0.028475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>smoothness error</td>\n",
              "      <td>0.003819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>compactness error</td>\n",
              "      <td>0.002594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>concavity error</td>\n",
              "      <td>0.003501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>concave points error</td>\n",
              "      <td>0.006780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>symmetry error</td>\n",
              "      <td>0.002520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>fractal dimension error</td>\n",
              "      <td>0.004063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>worst radius</td>\n",
              "      <td>0.105003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>worst texture</td>\n",
              "      <td>0.022585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>worst perimeter</td>\n",
              "      <td>0.134225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>worst area</td>\n",
              "      <td>0.113043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>worst smoothness</td>\n",
              "      <td>0.012033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>worst compactness</td>\n",
              "      <td>0.016953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>worst concavity</td>\n",
              "      <td>0.046426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>worst concave points</td>\n",
              "      <td>0.143496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>worst symmetry</td>\n",
              "      <td>0.007263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>worst fractal dimension</td>\n",
              "      <td>0.006607</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd344581-df23-4ce8-8862-b6cf0cf8dfd8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bd344581-df23-4ce8-8862-b6cf0cf8dfd8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bd344581-df23-4ce8-8862-b6cf0cf8dfd8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0d618582-e5d8-47ac-80ee-27849534c251\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0d618582-e5d8-47ac-80ee-27849534c251')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0d618582-e5d8-47ac-80ee-27849534c251 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_c6ac70aa-69d6-46a6-91ed-f7f71ce0ac12\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c6ac70aa-69d6-46a6-91ed-f7f71ce0ac12 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"Features\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"worst concave points\",\n          \"compactness error\",\n          \"worst area\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Feature Imporance Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.041962990748499164,\n        \"min\": 0.0020066540991853193,\n        \"max\": 0.14349587667376992,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          0.14349587667376992,\n          0.0025939439734597304,\n          0.11304301017484546\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree."
      ],
      "metadata": {
        "id": "6HIMoQDx-cVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_regression\n",
        "x, y = make_regression(n_samples=1000, n_features=10, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Training of Dataset\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=1)\n",
        "dt = DecisionTreeRegressor(random_state=1)\n",
        "\n",
        "rf.fit(x_train, y_train)\n",
        "dt.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred_rf = rf.predict(x_test)\n",
        "y_pred_dt = dt.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(f\"The mean_squared_error of the  random forest model is {mean_squared_error(y_test, y_pred_rf)}.\")\n",
        "print(f\"The mean_squared_error of the  single decision tree model is {mean_squared_error(y_test, y_pred_dt)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPSO4T5Y8Am3",
        "outputId": "49259611-767c-4f07-9421-4b512bef449f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean_squared_error of the  random forest model is 4082.7329997640004.\n",
            "The mean_squared_error of the  single decision tree model is 15485.574060694416.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compute the Out-of-Bag (0OB) score for a Random Forest Classifier."
      ],
      "metadata": {
        "id": "ST5Gbvp3_eGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "dt = load_breast_cancer()\n",
        "y = dt.target\n",
        "x = pd.DataFrame(dt.data, columns = dt.feature_names)\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(oob_score=True)\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "# Model Testing\n",
        "y_pred = rf.predict(x_test)\n",
        "\n",
        "# OBB Score\n",
        "print(f\"The OOB Score of the model is {rf.oob_score_}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33IogZID_Q_V",
        "outputId": "ff660aaf-3480-423c-8eb2-8128cf37ef40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The OOB Score of the model is 0.9494505494505494.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy."
      ],
      "metadata": {
        "id": "jOnTKjnLAs6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Training of Dataset\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "bgc = BaggingClassifier(estimator = SVC(), n_estimators=15, random_state=1)\n",
        "bgc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = bgc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The accuracy of the model is {accuracy_score(y_test, y_pred)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDebcPK_Alfq",
        "outputId": "f331f360-462a-4e38-9a5c-c653d2cd99dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is 0.872.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy."
      ],
      "metadata": {
        "id": "Hd3NbB_2BG_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model training with different trees\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf5 = RandomForestClassifier(n_estimators=5)\n",
        "rf10 = RandomForestClassifier(n_estimators=10)\n",
        "rf15 = RandomForestClassifier(n_estimators=15)\n",
        "rf20 = RandomForestClassifier(n_estimators=20)\n",
        "rf25 = RandomForestClassifier(n_estimators=25)\n",
        "\n",
        "rf5.fit(x_train, y_train)\n",
        "rf10.fit(x_train, y_train)\n",
        "rf15.fit(x_train, y_train)\n",
        "rf20.fit(x_train, y_train)\n",
        "rf25.fit(x_train, y_train)\n",
        "\n",
        "y_pred5 = rf5.predict(x_test)\n",
        "y_pred10 = rf10.predict(x_test)\n",
        "y_pred15 = rf15.predict(x_test)\n",
        "y_pred20 = rf20.predict(x_test)\n",
        "y_pred25 = rf25.predict(x_test)\n",
        "\n",
        "# Model Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The accuracy of the model with 5 trees is {accuracy_score(y_test, y_pred5)}.\")\n",
        "print(f\"The accuracy of the model with 10 trees is {accuracy_score(y_test, y_pred10)}.\")\n",
        "print(f\"The accuracy of the model with 15 trees is {accuracy_score(y_test, y_pred15)}.\")\n",
        "print(f\"The accuracy of the model with 20 trees is {accuracy_score(y_test, y_pred20)}.\")\n",
        "print(f\"The accuracy of the model with 25 trees is {accuracy_score(y_test, y_pred25)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyXzKKErBBA7",
        "outputId": "ca79d9df-6005-47eb-8ab4-1152f4f6df43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model with 5 trees is 0.88.\n",
            "The accuracy of the model with 10 trees is 0.852.\n",
            "The accuracy of the model with 15 trees is 0.888.\n",
            "The accuracy of the model with 20 trees is 0.876.\n",
            "The accuracy of the model with 25 trees is 0.872.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score."
      ],
      "metadata": {
        "id": "7lacIhvCDoTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Training of Dataset\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "bgc = BaggingClassifier(estimator = LogisticRegression(), n_estimators=15, random_state=1)\n",
        "bgc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = bgc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(f\"The AUC Score of the model is {roc_auc_score(y_test, y_pred)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fjDMTN1Dcrl",
        "outputId": "ad51f45c-7c8a-4b6c-83ac-b0773dfb3459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The AUC Score of the model is 0.8560523446019629.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores."
      ],
      "metadata": {
        "id": "p7YP9RQZEMPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_regression\n",
        "x, y = make_regression(n_samples=1000, n_features=10, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Training of Dataset\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "bgr = RandomForestRegressor()\n",
        "bgr.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = bgr.predict(x_test)\n",
        "\n",
        "bgr.feature_importances_"
      ],
      "metadata": {
        "id": "qFXwB9nRED8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f7e25da-57af-42c8-f923-7a7f6d742778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.02774392, 0.01149521, 0.01181876, 0.15388803, 0.22630914,\n",
              "       0.23402624, 0.02047628, 0.01122334, 0.02618346, 0.27683562])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "-cyImcv_ruV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "# Bagging\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "bgr = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=1)\n",
        "bgr.fit(x_train, y_train)\n",
        "\n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "# prediction\n",
        "y_pred_bgr = bgr.predict(x_test)\n",
        "y_pred_rfc = rfc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The accuracy score of the Bagging Model is {accuracy_score(y_test, y_pred_bgr)}.\")\n",
        "print(f\"The accuracy score of the Random Forest Model is {accuracy_score(y_test, y_pred_rfc)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai8N1jg0rczJ",
        "outputId": "6d303f1d-246b-4667-ca13-32c759b4763a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy score of the Bagging Model is 0.872.\n",
            "The accuracy score of the Random Forest Model is 0.88.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV."
      ],
      "metadata": {
        "id": "RigLTkClv9tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier()\n",
        "\n",
        "# Hyperparameters Tuning\n",
        "params = {\"n_estimators\":[10,20,50,100],\n",
        "          \"max_depth\":[10,11,12,13,14,15]}\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "clf = GridSearchCV(rfc, param_grid=params, verbose=3)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# Best Params\n",
        "print(f\"The best Parameters are {clf.best_params_}.\")"
      ],
      "metadata": {
        "id": "fkg8Bb4Yxc-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de499521-9363-4268-e823-3e7c564bce33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "[CV 1/5] END .....max_depth=10, n_estimators=10;, score=0.913 total time=   0.0s\n",
            "[CV 2/5] END .....max_depth=10, n_estimators=10;, score=0.827 total time=   0.0s\n",
            "[CV 3/5] END .....max_depth=10, n_estimators=10;, score=0.840 total time=   0.0s\n",
            "[CV 4/5] END .....max_depth=10, n_estimators=10;, score=0.820 total time=   0.0s\n",
            "[CV 5/5] END .....max_depth=10, n_estimators=10;, score=0.853 total time=   0.0s\n",
            "[CV 1/5] END .....max_depth=10, n_estimators=20;, score=0.907 total time=   0.1s\n",
            "[CV 2/5] END .....max_depth=10, n_estimators=20;, score=0.847 total time=   0.1s\n",
            "[CV 3/5] END .....max_depth=10, n_estimators=20;, score=0.860 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=10, n_estimators=20;, score=0.833 total time=   0.1s\n",
            "[CV 5/5] END .....max_depth=10, n_estimators=20;, score=0.820 total time=   0.1s\n",
            "[CV 1/5] END .....max_depth=10, n_estimators=50;, score=0.913 total time=   0.2s\n",
            "[CV 2/5] END .....max_depth=10, n_estimators=50;, score=0.833 total time=   0.2s\n",
            "[CV 3/5] END .....max_depth=10, n_estimators=50;, score=0.853 total time=   0.2s\n",
            "[CV 4/5] END .....max_depth=10, n_estimators=50;, score=0.867 total time=   0.2s\n",
            "[CV 5/5] END .....max_depth=10, n_estimators=50;, score=0.827 total time=   0.2s\n",
            "[CV 1/5] END ....max_depth=10, n_estimators=100;, score=0.927 total time=   0.5s\n",
            "[CV 2/5] END ....max_depth=10, n_estimators=100;, score=0.840 total time=   0.5s\n",
            "[CV 3/5] END ....max_depth=10, n_estimators=100;, score=0.860 total time=   0.3s\n",
            "[CV 4/5] END ....max_depth=10, n_estimators=100;, score=0.860 total time=   0.3s\n",
            "[CV 5/5] END ....max_depth=10, n_estimators=100;, score=0.833 total time=   0.3s\n",
            "[CV 1/5] END .....max_depth=11, n_estimators=10;, score=0.913 total time=   0.0s\n",
            "[CV 2/5] END .....max_depth=11, n_estimators=10;, score=0.820 total time=   0.0s\n",
            "[CV 3/5] END .....max_depth=11, n_estimators=10;, score=0.820 total time=   0.0s\n",
            "[CV 4/5] END .....max_depth=11, n_estimators=10;, score=0.860 total time=   0.0s\n",
            "[CV 5/5] END .....max_depth=11, n_estimators=10;, score=0.800 total time=   0.0s\n",
            "[CV 1/5] END .....max_depth=11, n_estimators=20;, score=0.933 total time=   0.1s\n",
            "[CV 2/5] END .....max_depth=11, n_estimators=20;, score=0.833 total time=   0.1s\n",
            "[CV 3/5] END .....max_depth=11, n_estimators=20;, score=0.873 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=11, n_estimators=20;, score=0.873 total time=   0.1s\n",
            "[CV 5/5] END .....max_depth=11, n_estimators=20;, score=0.833 total time=   0.1s\n",
            "[CV 1/5] END .....max_depth=11, n_estimators=50;, score=0.907 total time=   0.4s\n",
            "[CV 2/5] END .....max_depth=11, n_estimators=50;, score=0.840 total time=   0.3s\n",
            "[CV 3/5] END .....max_depth=11, n_estimators=50;, score=0.853 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=11, n_estimators=50;, score=0.867 total time=   0.2s\n",
            "[CV 5/5] END .....max_depth=11, n_estimators=50;, score=0.833 total time=   0.1s\n",
            "[CV 1/5] END ....max_depth=11, n_estimators=100;, score=0.933 total time=   0.3s\n",
            "[CV 2/5] END ....max_depth=11, n_estimators=100;, score=0.840 total time=   0.3s\n",
            "[CV 3/5] END ....max_depth=11, n_estimators=100;, score=0.853 total time=   0.3s\n",
            "[CV 4/5] END ....max_depth=11, n_estimators=100;, score=0.873 total time=   0.3s\n",
            "[CV 5/5] END ....max_depth=11, n_estimators=100;, score=0.820 total time=   0.3s\n",
            "[CV 1/5] END .....max_depth=12, n_estimators=10;, score=0.940 total time=   0.0s\n",
            "[CV 2/5] END .....max_depth=12, n_estimators=10;, score=0.827 total time=   0.0s\n",
            "[CV 3/5] END .....max_depth=12, n_estimators=10;, score=0.867 total time=   0.0s\n",
            "[CV 4/5] END .....max_depth=12, n_estimators=10;, score=0.840 total time=   0.0s\n",
            "[CV 5/5] END .....max_depth=12, n_estimators=10;, score=0.807 total time=   0.0s\n",
            "[CV 1/5] END .....max_depth=12, n_estimators=20;, score=0.920 total time=   0.1s\n",
            "[CV 2/5] END .....max_depth=12, n_estimators=20;, score=0.840 total time=   0.1s\n",
            "[CV 3/5] END .....max_depth=12, n_estimators=20;, score=0.867 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=12, n_estimators=20;, score=0.853 total time=   0.1s\n",
            "[CV 5/5] END .....max_depth=12, n_estimators=20;, score=0.820 total time=   0.1s\n",
            "[CV 1/5] END .....max_depth=12, n_estimators=50;, score=0.920 total time=   0.1s\n",
            "[CV 2/5] END .....max_depth=12, n_estimators=50;, score=0.833 total time=   0.1s\n",
            "[CV 3/5] END .....max_depth=12, n_estimators=50;, score=0.847 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=12, n_estimators=50;, score=0.860 total time=   0.2s\n",
            "[CV 5/5] END .....max_depth=12, n_estimators=50;, score=0.833 total time=   0.2s\n",
            "[CV 1/5] END ....max_depth=12, n_estimators=100;, score=0.927 total time=   0.4s\n",
            "[CV 2/5] END ....max_depth=12, n_estimators=100;, score=0.860 total time=   0.4s\n",
            "[CV 3/5] END ....max_depth=12, n_estimators=100;, score=0.853 total time=   0.4s\n",
            "[CV 4/5] END ....max_depth=12, n_estimators=100;, score=0.853 total time=   0.4s\n",
            "[CV 5/5] END ....max_depth=12, n_estimators=100;, score=0.827 total time=   0.4s\n",
            "[CV 1/5] END .....max_depth=13, n_estimators=10;, score=0.920 total time=   0.0s\n",
            "[CV 2/5] END .....max_depth=13, n_estimators=10;, score=0.847 total time=   0.0s\n",
            "[CV 3/5] END .....max_depth=13, n_estimators=10;, score=0.807 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=13, n_estimators=10;, score=0.840 total time=   0.1s\n",
            "[CV 5/5] END .....max_depth=13, n_estimators=10;, score=0.807 total time=   0.0s\n",
            "[CV 1/5] END .....max_depth=13, n_estimators=20;, score=0.920 total time=   0.1s\n",
            "[CV 2/5] END .....max_depth=13, n_estimators=20;, score=0.827 total time=   0.1s\n",
            "[CV 3/5] END .....max_depth=13, n_estimators=20;, score=0.847 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=13, n_estimators=20;, score=0.867 total time=   0.1s\n",
            "[CV 5/5] END .....max_depth=13, n_estimators=20;, score=0.820 total time=   0.1s\n",
            "[CV 1/5] END .....max_depth=13, n_estimators=50;, score=0.927 total time=   0.1s\n",
            "[CV 2/5] END .....max_depth=13, n_estimators=50;, score=0.853 total time=   0.1s\n",
            "[CV 3/5] END .....max_depth=13, n_estimators=50;, score=0.853 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=13, n_estimators=50;, score=0.860 total time=   0.1s\n",
            "[CV 5/5] END .....max_depth=13, n_estimators=50;, score=0.827 total time=   0.2s\n",
            "[CV 1/5] END ....max_depth=13, n_estimators=100;, score=0.933 total time=   0.3s\n",
            "[CV 2/5] END ....max_depth=13, n_estimators=100;, score=0.840 total time=   0.3s\n",
            "[CV 3/5] END ....max_depth=13, n_estimators=100;, score=0.853 total time=   0.3s\n",
            "[CV 4/5] END ....max_depth=13, n_estimators=100;, score=0.873 total time=   0.3s\n",
            "[CV 5/5] END ....max_depth=13, n_estimators=100;, score=0.827 total time=   0.3s\n",
            "[CV 1/5] END .....max_depth=14, n_estimators=10;, score=0.880 total time=   0.0s\n",
            "[CV 2/5] END .....max_depth=14, n_estimators=10;, score=0.867 total time=   0.0s\n",
            "[CV 3/5] END .....max_depth=14, n_estimators=10;, score=0.847 total time=   0.0s\n",
            "[CV 4/5] END .....max_depth=14, n_estimators=10;, score=0.873 total time=   0.0s\n",
            "[CV 5/5] END .....max_depth=14, n_estimators=10;, score=0.827 total time=   0.0s\n",
            "[CV 1/5] END .....max_depth=14, n_estimators=20;, score=0.900 total time=   0.1s\n",
            "[CV 2/5] END .....max_depth=14, n_estimators=20;, score=0.853 total time=   0.1s\n",
            "[CV 3/5] END .....max_depth=14, n_estimators=20;, score=0.833 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=14, n_estimators=20;, score=0.873 total time=   0.1s\n",
            "[CV 5/5] END .....max_depth=14, n_estimators=20;, score=0.827 total time=   0.1s\n",
            "[CV 1/5] END .....max_depth=14, n_estimators=50;, score=0.920 total time=   0.2s\n",
            "[CV 2/5] END .....max_depth=14, n_estimators=50;, score=0.853 total time=   0.1s\n",
            "[CV 3/5] END .....max_depth=14, n_estimators=50;, score=0.853 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=14, n_estimators=50;, score=0.867 total time=   0.1s\n",
            "[CV 5/5] END .....max_depth=14, n_estimators=50;, score=0.840 total time=   0.1s\n",
            "[CV 1/5] END ....max_depth=14, n_estimators=100;, score=0.940 total time=   0.3s\n",
            "[CV 2/5] END ....max_depth=14, n_estimators=100;, score=0.853 total time=   0.3s\n",
            "[CV 3/5] END ....max_depth=14, n_estimators=100;, score=0.860 total time=   0.3s\n",
            "[CV 4/5] END ....max_depth=14, n_estimators=100;, score=0.873 total time=   0.3s\n",
            "[CV 5/5] END ....max_depth=14, n_estimators=100;, score=0.820 total time=   0.3s\n",
            "[CV 1/5] END .....max_depth=15, n_estimators=10;, score=0.927 total time=   0.0s\n",
            "[CV 2/5] END .....max_depth=15, n_estimators=10;, score=0.833 total time=   0.0s\n",
            "[CV 3/5] END .....max_depth=15, n_estimators=10;, score=0.833 total time=   0.0s\n",
            "[CV 4/5] END .....max_depth=15, n_estimators=10;, score=0.833 total time=   0.0s\n",
            "[CV 5/5] END .....max_depth=15, n_estimators=10;, score=0.780 total time=   0.0s\n",
            "[CV 1/5] END .....max_depth=15, n_estimators=20;, score=0.927 total time=   0.1s\n",
            "[CV 2/5] END .....max_depth=15, n_estimators=20;, score=0.833 total time=   0.1s\n",
            "[CV 3/5] END .....max_depth=15, n_estimators=20;, score=0.847 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=15, n_estimators=20;, score=0.873 total time=   0.1s\n",
            "[CV 5/5] END .....max_depth=15, n_estimators=20;, score=0.827 total time=   0.1s\n",
            "[CV 1/5] END .....max_depth=15, n_estimators=50;, score=0.933 total time=   0.1s\n",
            "[CV 2/5] END .....max_depth=15, n_estimators=50;, score=0.853 total time=   0.1s\n",
            "[CV 3/5] END .....max_depth=15, n_estimators=50;, score=0.853 total time=   0.1s\n",
            "[CV 4/5] END .....max_depth=15, n_estimators=50;, score=0.873 total time=   0.2s\n",
            "[CV 5/5] END .....max_depth=15, n_estimators=50;, score=0.820 total time=   0.1s\n",
            "[CV 1/5] END ....max_depth=15, n_estimators=100;, score=0.907 total time=   0.3s\n",
            "[CV 2/5] END ....max_depth=15, n_estimators=100;, score=0.840 total time=   0.3s\n",
            "[CV 3/5] END ....max_depth=15, n_estimators=100;, score=0.860 total time=   0.3s\n",
            "[CV 4/5] END ....max_depth=15, n_estimators=100;, score=0.860 total time=   0.3s\n",
            "[CV 5/5] END ....max_depth=15, n_estimators=100;, score=0.820 total time=   0.3s\n",
            "The best Parameters are {'max_depth': 14, 'n_estimators': 100}.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance."
      ],
      "metadata": {
        "id": "m5QpLH6YxXnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_regression\n",
        "x, y = make_regression(n_samples=1000, n_features=10, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Training of Dataset\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "bgr10 = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=1)\n",
        "bgr20 = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=20, random_state=1)\n",
        "bgr50 = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=1)\n",
        "bgr100 = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100, random_state=1)\n",
        "\n",
        "bgr10.fit(x_train, y_train)\n",
        "bgr20.fit(x_train, y_train)\n",
        "bgr50.fit(x_train, y_train)\n",
        "bgr100.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred10 = bgr10.predict(x_test)\n",
        "y_pred20 = bgr20.predict(x_test)\n",
        "y_pred50 = bgr50.predict(x_test)\n",
        "y_pred100 = bgr100.predict(x_test)\n",
        "\n",
        "# Performance\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "print(f\"The RMSE of the Model with base estimators = 10 is {root_mean_squared_error(y_test, y_pred10)}.\")\n",
        "print(f\"The RMSE of the Model with base estimators = 20 is {root_mean_squared_error(y_test, y_pred20)}.\")\n",
        "print(f\"The RMSE of the Model with base estimators = 50 is {root_mean_squared_error(y_test, y_pred50)}.\")\n",
        "print(f\"The RMSE of the Model with base estimators = 100 is {root_mean_squared_error(y_test, y_pred100)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6iMh41_xI4V",
        "outputId": "aaf7aaab-1dac-4191-ab40-8d5a289a7097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The RMSE of the Model with base estimators = 10 is 69.8642680522377.\n",
            "The RMSE of the Model with base estimators = 20 is 68.57157415859758.\n",
            "The RMSE of the Model with base estimators = 50 is 64.56326035698595.\n",
            "The RMSE of the Model with base estimators = 100 is 63.85004165088308.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples."
      ],
      "metadata": {
        "id": "IDomaCoVEDVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier()\n",
        "\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "y_pred = rfc.predict(x_test)\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "misclassified_samples = x_test[misclassified_indices]\n",
        "misclassified_labels = y_test[misclassified_indices]\n",
        "predicted_labels = y_pred[misclassified_indices]\n",
        "\n",
        "# Print details of misclassified samples\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "for i in range(len(misclassified_indices)):\n",
        "    print(f\"Sample {misclassified_indices[i]} → True Label: {misclassified_labels[i]}, Predicted: {predicted_labels[i]}\")"
      ],
      "metadata": {
        "id": "HvL8RiqCD2A9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee96d4a0-3415-49b8-d5c5-52eb62dc2476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Misclassified Samples:\n",
            "Sample 11 → True Label: 1, Predicted: 0\n",
            "Sample 15 → True Label: 1, Predicted: 0\n",
            "Sample 21 → True Label: 1, Predicted: 0\n",
            "Sample 60 → True Label: 0, Predicted: 1\n",
            "Sample 70 → True Label: 1, Predicted: 0\n",
            "Sample 73 → True Label: 1, Predicted: 0\n",
            "Sample 78 → True Label: 0, Predicted: 1\n",
            "Sample 101 → True Label: 1, Predicted: 0\n",
            "Sample 106 → True Label: 0, Predicted: 1\n",
            "Sample 107 → True Label: 0, Predicted: 1\n",
            "Sample 114 → True Label: 1, Predicted: 0\n",
            "Sample 127 → True Label: 0, Predicted: 1\n",
            "Sample 131 → True Label: 0, Predicted: 1\n",
            "Sample 138 → True Label: 0, Predicted: 1\n",
            "Sample 147 → True Label: 0, Predicted: 1\n",
            "Sample 154 → True Label: 0, Predicted: 1\n",
            "Sample 155 → True Label: 0, Predicted: 1\n",
            "Sample 161 → True Label: 1, Predicted: 0\n",
            "Sample 164 → True Label: 0, Predicted: 1\n",
            "Sample 178 → True Label: 0, Predicted: 1\n",
            "Sample 179 → True Label: 1, Predicted: 0\n",
            "Sample 183 → True Label: 1, Predicted: 0\n",
            "Sample 185 → True Label: 1, Predicted: 0\n",
            "Sample 186 → True Label: 0, Predicted: 1\n",
            "Sample 196 → True Label: 0, Predicted: 1\n",
            "Sample 199 → True Label: 0, Predicted: 1\n",
            "Sample 204 → True Label: 0, Predicted: 1\n",
            "Sample 217 → True Label: 1, Predicted: 0\n",
            "Sample 219 → True Label: 0, Predicted: 1\n",
            "Sample 220 → True Label: 1, Predicted: 0\n",
            "Sample 222 → True Label: 1, Predicted: 0\n",
            "Sample 226 → True Label: 1, Predicted: 0\n",
            "Sample 227 → True Label: 0, Predicted: 1\n",
            "Sample 245 → True Label: 0, Predicted: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier."
      ],
      "metadata": {
        "id": "_c1yK6G_EM5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "# Bagging\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "bgc = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=1)\n",
        "bgc.fit(x_train, y_train)\n",
        "\n",
        "# Decision Tree\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred_bgc = bgc.predict(x_test)\n",
        "y_pred_dtc = dtc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The accuracy of Bagging Classifier is {accuracy_score(y_test, y_pred_bgc)}.\")\n",
        "print(f\"The accuracy of Decision Tree Classifier is {accuracy_score(y_test, y_pred_dtc)}.\")"
      ],
      "metadata": {
        "id": "1I0vulCHEOAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac8e432-d552-4046-e313-69199620e781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of Bagging Classifier is 0.872.\n",
            "The accuracy of Decision Tree Classifier is 0.796.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "k1CJYqfWodOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = rfc.predict(x_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4x57OC8oPO5",
        "outputId": "5665ee35-0629-43cf-a2a8-e17f0f5ecd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[116,  15],\n",
              "       [ 11, 108]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy."
      ],
      "metadata": {
        "id": "PfVUIEjkpLyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "base_learner = [(\"Decision Tree\", DecisionTreeClassifier()), (\"SVM\", SVC()), (\"Logistic Regression\", LogisticRegression())]\n",
        "\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "clf = StackingClassifier(estimators=base_learner, final_estimator=LogisticRegression())\n",
        "\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# Model Pred\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The Accuracy is {accuracy_score(y_test, y_pred)}.\")"
      ],
      "metadata": {
        "id": "NongRB3bpDwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3251ab0-7b67-4fb9-89a0-fe0c97946af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy is 0.876.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features."
      ],
      "metadata": {
        "id": "2gssDgtzpN0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "dt = load_breast_cancer()\n",
        "y = dt.target\n",
        "x = pd.DataFrame(dt.data, columns = dt.feature_names)\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "# Model Testing\n",
        "y_pred = rf.predict(x_test)\n",
        "\n",
        "# Feature_importance_Score\n",
        "\n",
        "df = {\"Features\": x.columns,\n",
        " \"Feature Imporance Score\": rf.feature_importances_}\n",
        "df = pd.DataFrame(df)\n",
        "df.sort_values(by='Feature Imporance Score', ascending=False).head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yeqxJLuIpSX2",
        "outputId": "5aa54ffd-104b-450a-af68-8838ac4e91c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Features  Feature Imporance Score\n",
              "27  worst concave points                 0.164192\n",
              "22       worst perimeter                 0.133334\n",
              "23            worst area                 0.095289\n",
              "20          worst radius                 0.084880\n",
              "3              mean area                 0.075034"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac651506-bca7-44f1-8d7c-d098c7d772b0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Features</th>\n",
              "      <th>Feature Imporance Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>worst concave points</td>\n",
              "      <td>0.164192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>worst perimeter</td>\n",
              "      <td>0.133334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>worst area</td>\n",
              "      <td>0.095289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>worst radius</td>\n",
              "      <td>0.084880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mean area</td>\n",
              "      <td>0.075034</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac651506-bca7-44f1-8d7c-d098c7d772b0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac651506-bca7-44f1-8d7c-d098c7d772b0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac651506-bca7-44f1-8d7c-d098c7d772b0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2a86c5bf-2421-4051-9e22-42137b9ee638\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a86c5bf-2421-4051-9e22-42137b9ee638')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2a86c5bf-2421-4051-9e22-42137b9ee638 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Features\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"worst perimeter\",\n          \"mean area\",\n          \"worst area\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Feature Imporance Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03724887038768224,\n        \"min\": 0.07503359377706051,\n        \"max\": 0.16419230138309254,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.13333441223509135,\n          0.07503359377706051,\n          0.09528871152360349\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score."
      ],
      "metadata": {
        "id": "m0eSw3zAqHBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "# Bagging\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "bgc = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=1)\n",
        "bgc.fit(x_train, y_train)\n",
        "\n",
        "y_pred = bgc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "print(f\"The Precision is {precision_score(y_test, y_pred)}.\")\n",
        "print(f\"The Recall is {recall_score(y_test, y_pred)}.\")\n",
        "print(f\"The f1 Score is {f1_score(y_test, y_pred)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiKASZfEpquS",
        "outputId": "0d74dbf9-db20-4b67-b10d-4fdf60d2ff7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Precision is 0.9065420560747663.\n",
            "The Recall is 0.8151260504201681.\n",
            "The f1 Score is 0.8584070796460177.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy."
      ],
      "metadata": {
        "id": "T43WWCPLrO52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training with Max depth 10\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(max_depth=10)\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = rfc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The Accuracy with Max depth 10 is {accuracy_score(y_test, y_pred)}.\")\n",
        "\n",
        "# Model Training with Max depth 20\n",
        "rfc = RandomForestClassifier(max_depth=20)\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = rfc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "print(f\"The Accuracy with Max depth 20 is {accuracy_score(y_test, y_pred)}.\")\n",
        "\n",
        "# Model Training with Max depth 50\n",
        "rfc = RandomForestClassifier(max_depth=50)\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = rfc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "print(f\"The Accuracy with Max depth 50 is {accuracy_score(y_test, y_pred)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLQb6XcTrDMn",
        "outputId": "589e5a6e-c208-4e75-fe8c-3fbea1b1da3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy with Max depth 10 is 0.88.\n",
            "The Accuracy with Max depth 20 is 0.884.\n",
            "The Accuracy with Max depth 50 is 0.872.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance."
      ],
      "metadata": {
        "id": "_SNYgwocsLsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "# Bagging with Decision Tree\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "bgc = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=1)\n",
        "bgc.fit(x_train, y_train)\n",
        "\n",
        "y_pred = bgc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The Accuracy with Decision Tree is {accuracy_score(y_test, y_pred)}.\")\n",
        "\n",
        "# Bagging with Kneighbour\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "bgc = BaggingClassifier(estimator=KNeighborsClassifier(), n_estimators=10, random_state=1)\n",
        "bgc.fit(x_train, y_train)\n",
        "\n",
        "y_pred = bgc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The Accuracy with KNeighbour is {accuracy_score(y_test, y_pred)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCAfkaWlr3-7",
        "outputId": "d7c15835-3d9d-4c07-f6a7-72c8e25c07c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy with Decision Tree is 0.872.\n",
            "The Accuracy with KNeighbour is 0.836.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score."
      ],
      "metadata": {
        "id": "ZbMQEWYhs7S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training with Max depth 10\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(max_depth=10)\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = rfc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(f\"The ROC-AUC Score with Max depth is {roc_auc_score(y_test, y_pred)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIV9HS1cs1i4",
        "outputId": "a73c1942-8622-4356-a516-608f2ff0b5d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ROC-AUC Score with Max depth is 0.8762909744050292.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validation"
      ],
      "metadata": {
        "id": "wXeM3MpVtWml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "# Bagging\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "bgc = BaggingClassifier()\n",
        "\n",
        "# Cross Validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\"estimator\":[DecisionTreeClassifier(), LogisticRegression(), SVC(), GaussianNB()],\n",
        "          \"n_estimators\":[10,20,30,50,100]}\n",
        "clf = GridSearchCV(bgc, param_grid=params, cv=5, verbose=1)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "print(f\"The best score is {clf.best_score_}.\")\n",
        "print(f\"The best Parameter is {clf.best_params_}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo8SuS9StPfd",
        "outputId": "e5cdd634-0c44-4172-88c5-3c300f1f2f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "The best score is 0.8560000000000001.\n",
            "The best Parameter is {'estimator': SVC(), 'n_estimators': 50}.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "EfFr2n4EvjYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training with Max depth 10\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(max_depth=10)\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = rfc.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title(\"Precision Recall Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BhcsVqFou7EQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "91450bf3-ecbe-4312-c4d1-88cd619b87ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy."
      ],
      "metadata": {
        "id": "8zMHVSCwFt0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "base_learner = [(\"Random Forest\", RandomForestClassifier())]\n",
        "\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "clf = StackingClassifier(estimators=base_learner, final_estimator=LogisticRegression())\n",
        "\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# Model Pred\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The Accuracy is {accuracy_score(y_test, y_pred)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6hGrjlwEfnh",
        "outputId": "c418fa0e-2a32-4037-b09d-18e7f1cfec7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy is 0.88.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "_603X7A8Gpb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Model Training\n",
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=5, random_state=42)\n",
        "\n",
        "# Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different bootstrap sample sizes\n",
        "bootstrap_sizes = [0.5, 0.7, 1.0]\n",
        "\n",
        "# Train Bagging Regressors with different bootstrap sizes and store results\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "results = []\n",
        "\n",
        "for bootstrap_fraction in bootstrap_sizes:\n",
        "    bagging_model = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=10,\n",
        "        max_samples=bootstrap_fraction,\n",
        "        random_state=42\n",
        "    )\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "results.append((bootstrap_fraction, mse, r2))\n",
        "print(f\"Bootstrap {bootstrap_fraction}: MSE = {mse:.4f}, R² = {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioqFr7ZhGTrc",
        "outputId": "f57f61e7-d354-478a-8e9f-cafd069ef6fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap 1.0: MSE = 3520.5830, R² = 0.7904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cSwkwHITHXH6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}